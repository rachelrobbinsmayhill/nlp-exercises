{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49588fcd-fd2e-4659-bda7-f0128e670fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb065a-4f2e-419e-aef4-4aff4bb3ed63",
   "metadata": {},
   "source": [
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both \n",
    "- the codeup blog articles \n",
    "- and the news articles that were previously acquired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20262b52-0cfc-4ad7-bbac-375dd2d4cd11",
   "metadata": {},
   "source": [
    "# ACQUIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3033b6f-5705-4714-bbdc-3a5af3aefb60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acquire' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jx/x5_xzwy107g6d0zd021r2ph40000gn/T/ipykernel_72183/3543583129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moriginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_article_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acquire' is not defined"
     ]
    }
   ],
   "source": [
    "original = acquire.get_article_text()\n",
    "print(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397adb7-2391-4b21-adfa-c5179cfb0907",
   "metadata": {},
   "source": [
    "# PREPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b3871-b64e-43fa-be69-3b9750465172",
   "metadata": {},
   "source": [
    "## 1. PREPARE - Convert text to all lower case for normalcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400205d9-acdb-4e7d-9d59-138b682c886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = original.lower()\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810cd41-eedd-4233-8392-bc3a7999f68a",
   "metadata": {},
   "source": [
    "## 2. PREPARE - Remove any accented characters, non-ASCII characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7fcc4-9acc-4b63-b2a7-86fc94d31a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = unicodedata.normalize('NFKD', article)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd5e4e-3cb1-4a74-b676-1b8699638c45",
   "metadata": {},
   "source": [
    "## 3. PREPARE - Remove special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4208b-461e-4562-8193-9192c858db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove anything that is not a through z, a number, a single quote, or whitespace\n",
    "article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a056f-71ca-43fb-83a9-85d362ccb5cd",
   "metadata": {},
   "source": [
    "### Tokenization - Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b967b-b219-4d62-8e88-e6a735aff02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(original, return_str=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897e393-578d-4c4b-869d-eb41ab09dccc",
   "metadata": {},
   "source": [
    "## 4. PREPARE - Stem or lemmatize the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dce9b5-4c90-43af-aa02-f1d148ebf4b5",
   "metadata": {},
   "source": [
    "### Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee50432-b23b-4f69-82dd-e6ddd6d02888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the nltk stemmer object, then use it\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "ps.stem('call'), ps.stem('called'), ps.stem('calling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0508e3-84ca-48ef-9fa5-b3240a62e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [ps.stem(word) for word in article.split()]\n",
    "article_stemmed = ' '.join(stems)\n",
    "print(article_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cfa92c-aece-455b-86a6-f8c2d8304ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(stems).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01799af0-5ee6-4dc3-a2bc-c01b6a631789",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9df737-a819-48de-b1b2-79118d5ce5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for word in 'study studies'.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61a64b-3059-4b81-aac6-508e365f5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "article_lemmatized = ' '.join(lemmas)\n",
    "\n",
    "print(article_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887d06e-ae29-4656-b66b-2db01ed71198",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lemmas).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0421b3-107c-405b-8957-8b9299ac97fc",
   "metadata": {},
   "source": [
    "## 5. PREPARE - Remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617af2bd-cb24-4ebb-be24-cd70efe5f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "stopword_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e840dc4-1387-432a-b841-c01f3fddf96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af0cf55-4f26-4f35-a963-972685ff30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = article.split()\n",
    "filtered_words = [w for w in words if w not in stopword_list]\n",
    "\n",
    "print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "print('---')\n",
    "\n",
    "article_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "print(article_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b9a89-27a5-47e2-98cd-c0b0b7c9dc16",
   "metadata": {},
   "source": [
    "## 6. PREPARE -  Store the clean text and the original text for use in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414aa30-fc1f-43fb-a7e0-1202db8ee47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e70f26a9-156f-4eb2-8771-6db1792ffc6e",
   "metadata": {},
   "source": [
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cf9d5-fe6e-4ef9-b648-14799268a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string, applies basic text cleaning to it,\n",
    "    then returns normalized text, making all text lowercase, normalizing unicode characters,\n",
    "    and replacing anything that is not a letter, number, whitespace, or a single quote.\n",
    "    '''\n",
    "    # removes accented characters; removes inconsistencies in unicode, converts resulting string to ASCII character, while ignoring warnings, and decodes to turn resulting bytes back into string. \n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "    # # removes special characters, substituting anything that is NOT a letter, number, apostrophe, or whitespace, then makes text lowercase\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string).lower()\n",
    "    return string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f732951-f430-432e-a0fc-3d6352a6624d",
   "metadata": {},
   "source": [
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16293128-73a1-44ef-8d30-e7301e0fc85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    tokenizes the string; breaking them down into discrete units.\n",
    "    '''\n",
    "    # Create tokenizer.\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # Use tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78684dd9-6708-4e09-bf64-560651fc56fb",
   "metadata": {},
   "source": [
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177721fc-c166-4b2c-98aa-ad74268c804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    '''This function accepts text and returns the stemmed text.\n",
    "    '''\n",
    "    # create the stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # apply the stemming transformation to all the words in the text using split\n",
    "    stems = [ps.stem(word) for word in article.split()]\n",
    "    \n",
    "    # join the list of words into a string again assigned to the variable article_stemmed\n",
    "    article_stemmed = ' '.join(stems)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b02790-9d04-4d76-b540-3972dc77d579",
   "metadata": {},
   "source": [
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660f548-7b25-440c-b235-1c4be5be3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    \n",
    "words = article.split()\n",
    "filtered_words = [w for w in words if w not in stopword_list]\n",
    "\n",
    "print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "print('---')\n",
    "\n",
    "article_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "print(article_without_stopwords)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf23e6-02d5-417e-a6f0-c1385be32674",
   "metadata": {},
   "source": [
    "5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154a0a5-849d-431f-973e-d989549b8a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stopword_list = stopwords.words('english')\n",
    "\n",
    "    stopword_list.remove('no')\n",
    "    stopword_list.remove('not')\n",
    "\n",
    "    stopword_list[:10]\n",
    "    \n",
    "    words = article.split()\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "\n",
    "    print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "    print('---')\n",
    "\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "    print(article_without_stopwords)\n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d2791-3170-42cb-aa5b-71270f5ab7df",
   "metadata": {},
   "source": [
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d33be-d460-4f3e-8ad5-ba0e43e95a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12665fd5-0a54-447f-81c2-51a4d2eb8584",
   "metadata": {},
   "source": [
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f091b1-4b5c-47ad-a5f4-fb9dbf0d4189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ce7564a-76c6-48e3-911d-b2d330264183",
   "metadata": {},
   "source": [
    "8.For each dataframe, produce the following columns:\n",
    "\n",
    "- title to hold the title\n",
    "- original to hold the original article/post content\n",
    "- clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "- stemmed to hold the stemmed version of the cleaned data.\n",
    "- lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76fedd8-0dfa-4eaa-829f-210bc50bbb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32a2e6ef-a64d-4f66-98bb-42ef1b850a06",
   "metadata": {},
   "source": [
    "9. Ask Yourself\n",
    "\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "     - Lemmatized text because it is a smaller dataset, and lemmatizing will result in more accurate identification of the 'meaning' of the word, identifying the lexicographically correct root word. \n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "    - It depends on the amount of time I have. Stemmed text could be better if short on time because it is a larger dataset, and lemmatizing, although it will result in more accurate identification of the 'meaning' of the word, it is considerably slower for larger datasets. However, if not short on time, I would want the most accurate results through lemmatized text. \n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n",
    "    - Stemmed text because it is a larger dataset, and lemmatizing, although it will result in more accurate identification of the 'meaning' of the word, it is considerably slower for larger datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd235ad4-5131-4b90-bc51-1e69c8eba8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
